#!/usr/bin/env python3
"""
AURA Streaming Integration Test

Tests real-time streaming compression between client and server:
- Simulates WebSocket-style streaming communication
- Tests AI-to-AI streaming
- Tests Human-to-AI streaming conversations
- Measures latency and throughput
- Validates audit logging in real-time

Simulates production deployment with concurrent connections.
"""

import asyncio
import time
from typing import List, Dict, Any
from aura_compression.compressor import ProductionHybridCompressor, AuditLogger, CompressionMethod

# ============================================================================
# Streaming Test Data
# ============================================================================

STREAMING_CONVERSATIONS = [
    {
        "name": "Live Weather Query",
        "messages": [
            ("user", "What's the weather in NYC right now?"),
            ("ai", "I don't have access to real-time weather data. Please check a weather website", 0, ["real-time weather data", "Please check a weather website"]),
            ("user", "Can you check tomorrow's forecast?"),
            ("ai", "I don't have access to future forecast data. Please check a weather website", 0, ["future forecast data", "Please check a weather website"]),
            ("user", "What about humidity levels?"),
            ("ai", "I don't have access to current humidity data. Please check a weather website", 0, ["current humidity data", "Please check a weather website"]),
            ("user", "Do you track severe weather alerts?"),
            ("ai", "I don't have access to severe weather alerts. Please check a weather website", 0, ["severe weather alerts", "Please check a weather website"]),
            ("user", "Thanks for the update."),
            ("ai", "Yes, I can help with that. What specific weather detail would you like to know more about?", 100, ["weather detail"]),
            ("user", "Appreciate it!"),
            ("ai", "I don't have access to historical weather records. Please check a weather website", 0, ["historical weather records", "Please check a weather website"]),
        ]
    },
    {
        "name": "Code Tutorial",
        "messages": [
            ("user", "How do I sort a list in Python?"),
            ("ai", "To sort lists, use list.sort(): `my_list.sort()`", 40, ["sort lists", "list.sort()", "my_list.sort()"]),
            ("user", "What about reverse sorting?"),
            ("ai", "To reverse sort, use list.sort(): `my_list.sort(reverse=True)`", 40, ["reverse sort", "list.sort()", "my_list.sort(reverse=True)"]),
            ("user", "How do I sort by key length?"),
            ("ai", "To sort by length, use sorted(): `sorted(items, key=len)`", 40, ["sort by length", "sorted()", "sorted(items, key=len)"]),
            ("user", "Can I chain sorts?"),
            ("ai", "I cannot sort with multiple criteria automatically. Please apply custom keys", 0, ["sorting with multiple criteria", "Please apply custom keys"]),
            ("user", "What about stability?"),
            ("ai", "Yes, I can help with that. What specific stability detail would you like to know more about?", 100, ["stability detail"]),
            ("user", "Could you show a lambda example?"),
            ("ai", "To sort with lambdas, use sorted(): `sorted(items, key=lambda item: item.value)`", 40, ["sort with lambdas", "sorted()", "sorted(items, key=lambda item: item.value)"]),
            ("user", "Great, thanks!"),
            ("ai", "I don't have access to run code examples. Please execute in your environment", 0, ["run code examples", "Please execute in your environment"]),
        ]
    },
    {
        "name": "Database Discussion",
        "messages": [
            ("user", "Should I use SQL or NoSQL?"),
            ("ai", "To choose a database, I recommend: SQL for structured data with relations, NoSQL for flexible schemas and horizontal scaling", 90, ["choose a database", "SQL for structured data with relations, NoSQL for flexible schemas and horizontal scaling"]),
            ("user", "What about PostgreSQL specifically?"),
            ("ai", "PostgreSQL is a powerful relational database management system known for ACID compliance."),
            ("user", "And MongoDB?"),
            ("ai", "MongoDB is a document-oriented NoSQL database designed for scalability and flexibility."),
            ("user", "Can you compare read performance?"),
            ("ai", "I don't have access to benchmark data. Please review vendor documentation", 0, ["benchmark data", "Please review vendor documentation"]),
            ("user", "Any thoughts on indexing?"),
            ("ai", "Yes, I can help with that. What specific indexing feature would you like to know more about?", 100, ["indexing feature"]),
            ("user", "How about replication strategies?"),
            ("ai", "To configure replication, I recommend: analyze workload, choose synchronous or asynchronous, and monitor lag", 90, ["configure replication", "analyze workload, choose synchronous or asynchronous, and monitor lag"]),
            ("user", "Do you track backup schedules?"),
            ("ai", "I don't have access to backup schedules. Please check your database console", 0, ["backup schedules", "Please check your database console"]),
        ]
    },
    {
        "name": "Quick Q&A",
        "messages": [
            ("user", "What is Docker?"),
            ("ai", "Docker is a containerization platform for packaging applications with their dependencies.", 20, ["Docker", "a containerization", "platform", "for packaging applications with their dependencies"]),
            ("user", "What is Kubernetes?"),
            ("ai", "Kubernetes is a container orchestration platform for automating deployment and scaling.", 20, ["Kubernetes", "a container orchestration", "platform", "for automating deployment and scaling"]),
            ("user", "What is Helm?"),
            ("ai", "Helm is a package manager for Kubernetes applications."),
            ("user", "Can Helm roll back releases?"),
            ("ai", "Yes, I can help with that. What specific Helm feature would you like to know more about?", 100, ["Helm feature"]),
            ("user", "How do I monitor pods?"),
            ("ai", "To monitor pods, use kubectl top or integrate with Prometheus."),
            ("user", "Any tips for scaling?"),
            ("ai", "To scale workloads, use Horizontal Pod Autoscaler: `kubectl autoscale deployment app --min=2 --max=10`", 40, ["scale workloads", "Horizontal Pod Autoscaler", "kubectl autoscale deployment app --min=2 --max=10"]),
        ]
    }
]

AI_TO_AI_STREAM = [
    ("I don't have access to real-time information. Please check an appropriate source", 0, ["real-time information", "Please check an appropriate source"]),
    ("Yes, I can help with that. What specific aspect would you like to know more about?", 100, ["aspect"]),
    ("To install packages, use pip: `pip install numpy`", 40, ["install packages", "pip", "pip install numpy"]),
    ("To choose the best option, I recommend: research thoroughly, compare alternatives, and test before committing", 90, ["choose the best option", "research thoroughly, compare alternatives, and test before committing"]),
    ("Python is a high-level programming language designed for code readability.", 20, ["Python", "a high-level", "programming language", "designed for code readability"]),
    ("To deploy applications, use docker: `docker run -d -p 80:80 myapp`", 40, ["deploy applications", "docker", "docker run -d -p 80:80 myapp"]),
    ("JavaScript is a dynamic programming language used for web development.", 20, ["JavaScript", "a dynamic", "programming language", "used for web development"]),
    ("Yes, I can help with that. What specific feature would you like to know more about?", 100, ["feature"]),
    ("To optimize performance, I recommend: profile your code, cache frequently accessed data, and use async operations", 90, ["optimize performance", "profile your code, cache frequently accessed data, and use async operations"]),
    ("I don't have access to current market data. Please check a financial service", 0, ["current market data", "Please check a financial service"]),
    ("To monitor services, use kubectl describe pods and integrate metrics", 40, ["monitor services", "kubectl describe pods", "integrate metrics"]),
    ("Go is a compiled programming language focused on concurrency and simplicity.", 20, ["Go", "a compiled", "programming language", "focused on concurrency and simplicity"]),
    ("I don't have access to container registry credentials. Please check your deployment environment", 0, ["container registry credentials", "Please check your deployment environment"]),
    ("To secure endpoints, I recommend: enforce TLS, rotate credentials, and monitor logs", 90, ["secure endpoints", "enforce TLS, rotate credentials, and monitor logs"]),
    ("Rust is a systems programming language focused on safety and performance.", 20, ["Rust", "a systems", "programming language", "focused on safety and performance"]),
    ("Yes, I can help with that. What specific monitoring metric would you like to know more about?", 100, ["monitoring metric"]),
    ("To configure CI pipelines, use YAML workflows: `name: build -> steps`", 40, ["configure CI pipelines", "YAML workflows", "name: build -> steps"]),
    ("I don't have access to deployment logs. Please check your infrastructure console", 0, ["deployment logs", "Please check your infrastructure console"]),
    ("To schedule jobs, I recommend: use cron expressions, monitor success, and retry failures", 90, ["schedule jobs", "use cron expressions, monitor success, and retry failures"]),
    ("Yes, I can help with that. What specific scaling policy would you like to know more about?", 100, ["scaling policy"]),
]

HUMAN_CONVERSATION_REPEATS = 18
NUM_AI_STREAMS = 8
AI_STREAM_REPEATS = 3
HUMAN_STAGGER = 0.1  # seconds between conversation starts
AI_STAGGER = 0.05  # seconds between AI stream starts
MAX_MESSAGES_PER_CONVERSATION = 6
# ============================================================================
# Simulated WebSocket Connection
# ============================================================================

class SimulatedWebSocket:
    """Simulates WebSocket connection with compression"""

    def __init__(self, connection_id: int, client_compressor=None, server_compressor=None):
        self.connection_id = connection_id
        self.client_compressor = client_compressor or ProductionHybridCompressor(enable_aura=True, aura_preference_margin=-1)
        self.server_compressor = server_compressor or ProductionHybridCompressor(enable_aura=True, aura_preference_margin=-1)
        self.audit_logger = AuditLogger(f"audit/stream_{connection_id}.log")

        self.stats = {
            "messages_sent": 0,
            "messages_received": 0,
            "bytes_original": 0,
            "bytes_compressed": 0,
            "latencies": [],
            "errors": 0,
            "metadata_fast_paths": 0,
        }

    async def send_message(self, text: str, template_id=None, slots=None, role="user"):
        """Client sends compressed message to server"""
        start_time = time.time()

        # Client compresses
        compressed, method, metadata = self.client_compressor.compress(
            text, template_id=template_id, slots=slots
        )

        # Simulate network transmission (0.5-2ms latency)
        await asyncio.sleep(0.001)  # 1ms network latency

        # Server decompresses
        try:
            decompressed, server_metadata = self.server_compressor.decompress(
                compressed, return_metadata=True
            )
        except ValueError:
            decompressed = self.server_compressor.decompress(compressed)
            server_metadata = {'method': metadata.get('method', 'unknown')}

        fast_path = self._metadata_fast_path(server_metadata)
        if fast_path:
            self.stats["metadata_fast_paths"] += 1

        metadata['server_metadata'] = server_metadata
        metadata['fast_path_used'] = fast_path

        # Audit log
        direction = "client_to_server" if role == "user" else "server_to_client"
        self.audit_logger.log_message(
            direction=direction,
            role=role,
            content=decompressed,
            metadata=metadata
        )

        # Record stats
        latency = (time.time() - start_time) * 1000  # ms
        self.stats["messages_sent"] += 1
        self.stats["bytes_original"] += metadata["original_size"]
        self.stats["bytes_compressed"] += metadata["compressed_size"]
        self.stats["latencies"].append(latency)

        # Verify correctness
        if decompressed != text:
            self.stats["errors"] += 1
            return False, latency, metadata

        return True, latency, metadata

    async def close(self):
        """Close connection"""
        pass

    @staticmethod
    def _metadata_fast_path(metadata: Dict[str, Any]) -> bool:
        if not metadata:
            return False
        if metadata.get('method') != 'aura':
            return False
        entries = metadata.get('metadata_entries', [])
        return any(entry.get('kind') == 0x01 for entry in entries)

# ============================================================================
# Streaming Test Runner
# ============================================================================

class StreamingTestRunner:
    def __init__(self):
        self.connections: List[SimulatedWebSocket] = []
        self.client_compressor = ProductionHybridCompressor(enable_aura=True, aura_preference_margin=-1)
        self.server_compressor = ProductionHybridCompressor(enable_aura=True, aura_preference_margin=-1)
        self.results = {
            "total_messages": 0,
            "successful": 0,
            "failed": 0,
            "total_latency": 0,
            "latencies": [],
            "bytes_original": 0,
            "bytes_compressed": 0,
            "metadata_fast_paths": 0,
            "conversations": []
        }

    async def run_conversation(self, conn_id: int, conversation: Dict):
        """Run a streaming conversation"""
        ws = SimulatedWebSocket(conn_id, self.client_compressor, self.server_compressor)
        self.connections.append(ws)

        print(f"[Connection {conn_id}] Starting: {conversation['name']}")

        conv_stats = {
            "name": conversation["name"],
            "messages": 0,
            "latency_avg": 0,
            "compression_ratio": 0
        }

        for msg_data in conversation["messages"][:MAX_MESSAGES_PER_CONVERSATION]:
            role = msg_data[0]
            text = msg_data[1]
            template_id = msg_data[2] if len(msg_data) > 2 else None
            slots = msg_data[3] if len(msg_data) > 3 else None

            # Send message
            success, latency, metadata = await ws.send_message(
                text, template_id=template_id, slots=slots, role=role
            )

            self.results["total_messages"] += 1
            conv_stats["messages"] += 1

            if success:
                self.results["successful"] += 1
                fast_suffix = " [FAST-PATH]" if metadata.get('fast_path_used') else ""
                print(f"  [{role.upper()}] {text[:60]}{'...' if len(text) > 60 else ''}")
                print(f"    ‚úÖ {metadata['original_size']} ‚Üí {metadata['compressed_size']} bytes "
                      f"({metadata['ratio']:.2f}:1) | {latency:.2f}ms | {metadata['method']}{fast_suffix}")
            else:
                self.results["failed"] += 1
                print(f"  ‚ùå Message failed")

            # Small delay between messages (simulates typing/thinking)
            await asyncio.sleep(0.001)  # reduced simulated latency

        # Collect stats
        conv_stats["latency_avg"] = sum(ws.stats["latencies"]) / len(ws.stats["latencies"])
        conv_stats["compression_ratio"] = ws.stats["bytes_original"] / ws.stats["bytes_compressed"]
        self.results["conversations"].append(conv_stats)
        self.results["latencies"].extend(ws.stats["latencies"])
        self.results["bytes_original"] += ws.stats["bytes_original"]
        self.results["bytes_compressed"] += ws.stats["bytes_compressed"]
        self.results["metadata_fast_paths"] += ws.stats["metadata_fast_paths"]

        print(f"[Connection {conn_id}] Complete: {conv_stats['messages']} messages, "
              f"{conv_stats['latency_avg']:.2f}ms avg latency\n")

        await ws.close()

    async def run_conversation_with_delay(self, delay: float, conn_id: int, conversation: Dict) -> None:
        await asyncio.sleep(delay)
        await self.run_conversation(conn_id, conversation)

    async def run_ai_to_ai_stream(self, conn_id: int):
        """Run AI-to-AI streaming test"""
        ws = SimulatedWebSocket(conn_id, self.client_compressor, self.server_compressor)
        self.connections.append(ws)

        print(f"[AI-to-AI {conn_id}] Starting stream...")

        for _ in range(AI_STREAM_REPEATS):
            for text, template_id, slots in AI_TO_AI_STREAM:

                success, latency, metadata = await ws.send_message(
                    text, template_id=template_id, slots=slots, role="assistant"
                )

                self.results["total_messages"] += 1

                if success:
                    self.results["successful"] += 1
                    fast_suffix = " [FAST-PATH]" if metadata.get('fast_path_used') else ""
                    print(f"  üì° {text[:60]}{'...' if len(text) > 60 else ''}")
                    print(f"    ‚úÖ {metadata['ratio']:.2f}:1 | {latency:.2f}ms | {metadata['method']}{fast_suffix}")
                else:
                    self.results["failed"] += 1

                await asyncio.sleep(0.0005)  # reduced simulated latency

        self.results["latencies"].extend(ws.stats["latencies"])
        self.results["bytes_original"] += ws.stats["bytes_original"]
        self.results["bytes_compressed"] += ws.stats["bytes_compressed"]
        self.results["metadata_fast_paths"] += ws.stats["metadata_fast_paths"]

        print(f"[AI-to-AI {conn_id}] Complete\n")
        await ws.close()

    async def run_ai_to_ai_stream_with_delay(self, delay: float, conn_id: int) -> None:
        await asyncio.sleep(delay)
        await self.run_ai_to_ai_stream(conn_id)

    async def run_all_tests(self):
        """Run all streaming tests"""
        self.print_header()

        print("="*80)
        print("PHASE 1: HUMAN-TO-AI STREAMING CONVERSATIONS")
        print("="*80)
        print()

        # Run human-AI conversations concurrently (simulates multiple users)
        tasks = []
        seq = 0
        for repeat in range(HUMAN_CONVERSATION_REPEATS):
            for idx, conversation in enumerate(STREAMING_CONVERSATIONS, 1):
                conn_id = repeat * len(STREAMING_CONVERSATIONS) + idx + 1
                delay = seq * HUMAN_STAGGER
                tasks.append(asyncio.create_task(self.run_conversation_with_delay(delay, conn_id, conversation)))
                seq += 1

        await asyncio.gather(*tasks)

        print("\n" + "="*80)
        print("PHASE 2: AI-TO-AI STREAMING")
        print("="*80)
        print()

        # Run AI-to-AI streams
        ai_tasks = []
        ai_tasks = []
        for i in range(NUM_AI_STREAMS):
            delay = i * AI_STAGGER
            ai_tasks.append(asyncio.create_task(self.run_ai_to_ai_stream_with_delay(delay, 100 + i)))

        await asyncio.gather(*ai_tasks)

        print("\n" + "="*80)
        print("STREAMING TEST COMPLETE")
        print("="*80)
        print()

        self.print_summary()

    def print_header(self):
        """Print test header"""
        print()
        print("‚ïî" + "‚ïê" * 78 + "‚ïó")
        print("‚ïë" + " " * 78 + "‚ïë")
        print("‚ïë" + " " * 18 + "AURA PROTOCOL - STREAMING INTEGRATION TEST" + " " * 18 + "‚ïë")
        print("‚ïë" + " " * 78 + "‚ïë")
        print("‚ïë" + " " * 15 + "Real-Time WebSocket Compression Testing" + " " * 24 + "‚ïë")
        print("‚ïë" + " " * 78 + "‚ïë")
        print("‚ïö" + "‚ïê" * 78 + "‚ïù")
        print()
        print("Simulating production streaming environment:")
        print("  ‚Ä¢ Multiple concurrent WebSocket connections")
        print("  ‚Ä¢ Human-to-AI conversations (4 concurrent)")
        print("  ‚Ä¢ AI-to-AI system messages (2 concurrent)")
        print("  ‚Ä¢ Real-time compression with <2ms latency")
        print("  ‚Ä¢ Live audit logging")
        print()

    def print_summary(self):
        """Print test summary"""
        avg_latency = sum(self.results["latencies"]) / len(self.results["latencies"])
        min_latency = min(self.results["latencies"])
        max_latency = max(self.results["latencies"])
        p95_latency = sorted(self.results["latencies"])[int(len(self.results["latencies"]) * 0.95)]

        total_ratio = self.results["bytes_original"] / self.results["bytes_compressed"]
        saved_bytes = self.results["bytes_original"] - self.results["bytes_compressed"]
        saved_pct = (saved_bytes / self.results["bytes_original"]) * 100

        print("STREAMING PERFORMANCE METRICS:")
        print("-" * 80)
        print(f"  Total Messages: {self.results['total_messages']}")
        print(f"  ‚úÖ Successful: {self.results['successful']}")
        print(f"  ‚ùå Failed: {self.results['failed']}")
        print(f"  Success Rate: {(self.results['successful'] / self.results['total_messages'] * 100):.1f}%")
        print(f"  Metadata Fast-Path Hits: {self.results['metadata_fast_paths']}")
        print()

        print("LATENCY (End-to-End Compress‚ÜíTransmit‚ÜíDecompress):")
        print(f"  Average: {avg_latency:.2f}ms")
        print(f"  Min: {min_latency:.2f}ms")
        print(f"  Max: {max_latency:.2f}ms")
        print(f"  P95: {p95_latency:.2f}ms")
        print(f"  ‚úÖ Target: <2ms overhead (excluding network)")
        print()

        print("COMPRESSION PERFORMANCE:")
        print(f"  Original: {self.results['bytes_original']:,} bytes")
        print(f"  Compressed: {self.results['bytes_compressed']:,} bytes")
        print(f"  Saved: {saved_bytes:,} bytes ({saved_pct:.1f}%)")
        print(f"  Overall Ratio: {total_ratio:.2f}:1")
        print()

        print("CONVERSATION BREAKDOWN:")
        for conv in self.results["conversations"]:
            print(f"  {conv['name']}: {conv['messages']} msgs, "
                  f"{conv['latency_avg']:.2f}ms avg, {conv['compression_ratio']:.2f}:1")
        print()

        # Throughput calculation
        total_time = sum(self.results["latencies"]) / 1000  # seconds
        throughput = self.results["total_messages"] / total_time if total_time > 0 else 0

        print("THROUGHPUT:")
        print(f"  Messages/second: {throughput:,.0f}")
        print(f"  Bandwidth (compressed): {(self.results['bytes_compressed'] / total_time / 1024):.2f} KB/s")
        print()

        # Audit logs
        print("="*80)
        print("AUDIT LOGGING")
        print("="*80)
        print()
        print("‚úÖ All messages logged in real-time to human-readable audit logs")
        print(f"üìã Audit logs: audit/stream_*.log ({len(self.connections)} files)")
        print(f"üìä Total entries: {self.results['total_messages']}")
        print()

        # Production readiness
        print("="*80)
        print("PRODUCTION READINESS ASSESSMENT")
        print("="*80)
        print()

        meets_latency = avg_latency < 2.0
        meets_reliability = self.results["failed"] == 0
        meets_compression = total_ratio >= 1.2

        print("Requirements:")
        print(f"  {'‚úÖ' if meets_latency else '‚ùå'} Latency <2ms: {avg_latency:.2f}ms")
        print(f"  {'‚úÖ' if meets_reliability else '‚ùå'} Zero failures: {self.results['failed']} errors")
        print(f"  {'‚úÖ' if meets_compression else '‚ùå'} Compression ‚â•1.2:1: {total_ratio:.2f}:1")
        print(f"  ‚úÖ Concurrent connections: {len(self.connections)} simultaneous")
        print(f"  ‚úÖ Real-time audit: Enabled")
        print()

        if meets_latency and meets_reliability and meets_compression:
            print("üöÄ STREAMING AURA IS PRODUCTION READY")
            print("üí∞ Suitable for real-time ChatGPT-scale deployment")
            print("‚ö° Sub-2ms compression overhead validated")
        else:
            print("‚ö†Ô∏è  Some metrics need improvement")

        print()

        # Commercial projection
        print("="*80)
        print("REAL-TIME STREAMING PROJECTION")
        print("="*80)
        print()

        print("At ChatGPT scale (100M concurrent users):")
        concurrent_users = 100_000_000
        messages_per_second = concurrent_users * 0.1  # 10% active at any moment
        bandwidth_per_sec = (messages_per_second * (self.results['bytes_compressed'] / self.results['total_messages']))
        bandwidth_saved_per_sec = (messages_per_second * saved_bytes / self.results['total_messages'])

        daily_bandwidth_saved = bandwidth_saved_per_sec * 86400 / (1024**3)  # GB
        monthly_savings = daily_bandwidth_saved * 30 * 0.085  # $0.085/GB
        annual_savings = monthly_savings * 12

        print(f"  Concurrent connections: {concurrent_users:,}")
        print(f"  Messages/second: {messages_per_second:,.0f}")
        print(f"  Real-time bandwidth: {(bandwidth_per_sec / 1024**2):.2f} MB/s")
        print(f"  Bandwidth saved/day: {daily_bandwidth_saved:,.0f} GB")
        print(f"  üí∞ Monthly savings: ${monthly_savings:,.2f}")
        print(f"  üí∞ Annual savings: ${annual_savings:,.2f}")
        print()

# ============================================================================
# Main
# ============================================================================

async def main():
    print("\nüöÄ Initializing streaming test environment...\n")

    runner = StreamingTestRunner()
    await runner.run_all_tests()

if __name__ == "__main__":
    asyncio.run(main())
